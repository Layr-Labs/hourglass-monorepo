# GPU Executor Values Example
# This example shows configuration for ML/AI workloads requiring GPU resources

executor:
  name: "gpu-executor"
  replicaCount: 1
  
  # Higher resources for GPU workloads
  resources:
    requests:
      cpu: "4"
      memory: "8Gi"
    limits:
      cpu: "16"
      memory: "32Gi"

aggregator:
  endpoint: "aggregator.ml-project.com:9090"
  tls:
    enabled: false

chains:
  ethereum:
    enabled: true
    rpcUrl: "https://eth-mainnet.alchemyapi.io/v2/ML_PROJECT_KEY"
    taskMailboxAddress: "0x1111222233334444555566667777888899990000"

# GPU-optimized AVS configuration
avs:
  supportedAvs:
    - address: "0x1111222233334444555566667777888899990000"
      name: "ml-avs"
      performer:
        image: "ml-project/gpu-performer"
        version: "v3.0.0"
      
      # GPU resource requirements
      resources:
        requests:
          nvidia.com/gpu: "1"
          cpu: "4"
          memory: "8Gi"
        limits:
          nvidia.com/gpu: "1"
          cpu: "16"
          memory: "32Gi"
      
      # Hardware requirements for GPU
      hardware:
        gpu:
          required: true
          type: "nvidia-a100"  # or nvidia-v100, nvidia-t4, etc.
          count: 1
      
      # GPU-specific scheduling
      scheduling:
        nodeSelector:
          accelerator: "nvidia-tesla-a100"
          # Alternative node selectors:
          # kubernetes.io/arch: amd64
          # node.kubernetes.io/instance-type: "p3.2xlarge"
        
        tolerations:
          - key: "nvidia.com/gpu"
            operator: "Exists"
            effect: "NoSchedule"
          - key: "dedicated"
            operator: "Equal"
            value: "gpu-workloads"
            effect: "NoSchedule"
        
        # GPU workload priority
        priorityClassName: "gpu-priority"
        
        # Use GPU-optimized runtime (if available)
        runtimeClass: "nvidia"

secrets:
  operatorKeys:
    ecdsaPrivateKey: "LS0tLS1CRUdJTiBQUklWQVRFIEtFWS0tLS0t..."

# Large storage for ML models and datasets
persistence:
  enabled: true
  storageClass: "fast-ssd"
  size: "500Gi"  # Large storage for ML workloads

# GPU-optimized scheduling
scheduling:
  nodeSelector:
    accelerator: "nvidia-tesla-a100"
  
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
    - key: "dedicated"
      operator: "Equal"
      value: "gpu-workloads"
      effect: "NoSchedule"

# Enhanced performer configuration for GPU workloads
performer:
  # GPU performers may take longer to start
  startupTimeout: "600s"  # 10 minutes
  connectionTimeout: "60s"
  maxPerformers: 5  # Fewer performers due to GPU constraints
  
  # More frequent health checks for expensive GPU resources
  healthCheckInterval: "15s"
  resourceMonitoringEnabled: true
  resourceMonitoringInterval: "30s"

# Extended health checks for GPU initialization
healthChecks:
  startupProbe:
    initialDelaySeconds: 60
    periodSeconds: 15
    failureThreshold: 40  # Allow up to 10 minutes for startup
  
  readinessProbe:
    initialDelaySeconds: 30
    periodSeconds: 10
  
  livenessProbe:
    initialDelaySeconds: 120
    periodSeconds: 30

# Monitoring for GPU workloads
metrics:
  enabled: true
  serviceMonitor:
    enabled: true

# Network policy allowing ML traffic
networkPolicy:
  enabled: true
  egress:
    # Allow access to ML model repositories
    - to: []
      ports:
        - protocol: TCP
          port: 443  # HTTPS for model downloads
        - protocol: TCP
          port: 80   # HTTP for some repositories